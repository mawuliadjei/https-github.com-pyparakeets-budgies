{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Template Notebook - v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should serve as a template for the standard modelling we will all be using for as the base of version 1 development.\n",
    "\n",
    "The goal is for us to use the same predicted probabilities and classes to build different functions according the to tasks assigned to us.\n",
    "\n",
    "The notebook should be used as follows:\n",
    "\n",
    "1. Import all needed packages:\n",
    "2. Load Data\n",
    "3. Split Train and Test Sets\n",
    "4. Build and Run model\n",
    "5. Output predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the case a Module Not Found error is thrown, it will automatically attempt to install all needed packages. You would have to rerun the import package cell again after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_packages():\n",
    "    !pip install pandas\n",
    "    !pip install numpy\n",
    "    !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "import successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import datasets\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    print('import successful')\n",
    "except ModuleNotFoundError:\n",
    "    print('forcing install of necessary packages. If you see this, rerun this cell to try again')\n",
    "    install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we're starting off with is the standard breast cancer dataset that lends itself directly to binary classification modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the simplest flavour of the ordinary least squares logistic regression in this case and no data transformation / scaling to maintain simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides the basic outputs we will be using for function building as variables. They are as follows:\n",
    "\n",
    "1. predicted_probabilities: this is output of pre for which we will be performing optizations. This has two 'columns' where the values in index 0 are the probabilities for the class 0 and those in index 1 have probabilies for class 1\n",
    "\n",
    "2. sklearn_class_labels: this is the class label prediction computed by sklearn's density function\n",
    "\n",
    "3. naive_class_labels: this is the class label prediction computed by using a naïve 0.5 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of predicted probabilties: (188, 2)\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = clf.predict_proba(X_test)\n",
    "print('shape of predicted probabilties:', predicted_probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of model default class predicttions: (188,)\n"
     ]
    }
   ],
   "source": [
    "sklearn_class_labels = clf.predict(X_test)\n",
    "print('shape of model default class predicttions:', sklearn_class_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sklearn_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of naïve class predicttions: (188,)\n"
     ]
    }
   ],
   "source": [
    "naive_class_labels = np.where(predicted_probabilities[:,1] > 0.5, 1, 0)\n",
    "print('shape of naïve class predicttions:', naive_class_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy and F1 score of sklearn predicted classes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9574468085106383, 0.9669421487603306)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "print('Accuracy and F1 score of sklearn predicted classes')\n",
    "accuracy_score(sklearn_class_labels, y_test), f1_score(sklearn_class_labels, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy and F1 score of naive predicted classes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9574468085106383, 0.9669421487603306)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "print('Accuracy and F1 score of naive predicted classes')\n",
    "accuracy_score(naive_class_labels, y_test), f1_score(naive_class_labels, y_test)"
   ]
  },
  {
   "source": [
    "# Function for optimum accuracy threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.17455829127349312"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Get_Accuracy:\n",
    "    def __init__(self, y_test, predicted_prob):\n",
    "\n",
    "        #getting probability\n",
    "        pred_prob = predicted_prob[:, 1]\n",
    "\n",
    "        #geting the accuracy and threshold using the probabilities\n",
    "        accs = []\n",
    "        for threshold in np.unique(pred_prob):\n",
    "            accuracies = (accuracy_score(y_test, pred_prob > threshold), threshold)\n",
    "            accs.append(accuracies)\n",
    "\n",
    "        #getting the highest accuracy at the best threshold soriing from increasing numerical order\n",
    "        self.accuracy, self.threshold = (max(accs, key = lambda pair: pair[0]))\n",
    "accuracy = Get_Accuracy(y_test, predicted_probabilities)\n",
    "\n",
    "#Trial\n",
    "accuracy.threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}