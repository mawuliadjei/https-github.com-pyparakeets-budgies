{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Template Notebook - v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should serve as a template for the standard modelling we will all be using for as the base of version 1 development.\n",
    "\n",
    "The goal is for us to use the same predicted probabilities and classes to build different functions according the to tasks assigned to us.\n",
    "\n",
    "The notebook should be used as follows:\n",
    "1. Import all needed packages:\n",
    "2. Load Data\n",
    "3. Split Train and Test Sets\n",
    "4. Build and Run model\n",
    "5. Output predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the case a Module Not Found error is thrown, it will automatically attempt to install all needed packages. You would have to rerun the import package cell again after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_packages():\n",
    "    !pip install pandas\n",
    "    !pip install numpy\n",
    "    !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import datasets\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    print('import successful')\n",
    "except ModuleNotFoundError:\n",
    "    print('forcing install of necessary packages. If you see this, rerun this cell to try again')\n",
    "    install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we're starting off with is the standard breast cancer dataset that lends itself directly to binary classification modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the simplest flavour of the ordinary least squares logistic regression in this case and no data transformation / scaling to maintain simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliadjei/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides the basic outputs we will be using for function building as variables. They are as follows:\n",
    "1. predicted_probabilities: this is output of pre for which we will be performing optizations. This has two 'columns' where the values in index 0 are the probabilities for the class 0 and those in index 1 have probabilies for class 1\n",
    "2. sklearn_class_labels: this is the class label prediction computed by sklearn's density function\n",
    "3. naive_class_labels: this is the class label prediction computed by using a naïve 0.5 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of predicted probabilties: (188, 2)\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = clf.predict_proba(X_test)\n",
    "print('shape of predicted probabilties:', predicted_probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of model default class predicttions: (188,)\n"
     ]
    }
   ],
   "source": [
    "sklearn_class_labels = clf.predict(X_test)\n",
    "print('shape of model default class predicttions:', sklearn_class_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of naïve class predicttions: (188,)\n"
     ]
    }
   ],
   "source": [
    "naive_class_labels = np.where(predicted_probabilities[:,1] > 0.5, 1, 0)\n",
    "print('shape of naïve class predicttions:', naive_class_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and F1 score of sklearn predicted classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9680851063829787, 0.9752066115702479)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy and F1 score of sklearn predicted classes')\n",
    "accuracy_score(sklearn_class_labels, y_test), f1_score(sklearn_class_labels, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and F1 score of naive predicted classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9680851063829787, 0.9752066115702479)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy and F1 score of naive predicted classes')\n",
    "accuracy_score(naive_class_labels, y_test), f1_score(naive_class_labels, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_space(number_of_values: int =100) -> np.array:\n",
    "    return np.linspace(0,1,number_of_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes(predicted_probabilities: np.array,\n",
    "                    threshold: float,\n",
    "                    is_multidimensional: bool,\n",
    "                    target_class_index: int = None):\n",
    "    if is_multidimensional:\n",
    "        assert target_class_index is not None\n",
    "        assert len(predicted_probabilities) > 1\n",
    "    if is_multidimensional:\n",
    "        classes = np.where(predicted_probabilities[:,target_class_index] >= threshold, 1, 0)\n",
    "    else:\n",
    "        classes = np.where(predicted_probabilities >= threshold, 1, 0)\n",
    "    return classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = convert_classes(predicted_probabilities=predicted_probabilities,\n",
    "                          threshold=0.5, \n",
    "                          is_multidimensional=True,\n",
    "                          target_class_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_f1(predicted_probabilities: np.array,\n",
    "                       is_multidimensional: bool,\n",
    "                       search_space: np.array, # this should be type unioned with lists\\\n",
    "                       y_test: np.array,\n",
    "                       target_class_index: int = None):\n",
    "    f1_scores = []\n",
    "    for i in search_space:\n",
    "        classes = convert_classes(predicted_probabilities=predicted_probabilities,\n",
    "                                  threshold=i, \n",
    "                                  is_multidimensional=is_multidimensional,\n",
    "                                  target_class_index=target_class_index)\n",
    "        f1_scores.append(f1_score(classes, y_test))\n",
    "    best_f1_score = max(f1_scores)\n",
    "    best_f1_index = f1_scores.index(best_f1_score)\n",
    "    best_f1_threshold = search_space[best_f1_index]\n",
    "    print(f'best f1 score: {best_f1_score} occurs at threshold {best_f1_threshold}')\n",
    "    return best_f1_score, best_f1_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sensitivity(predicted_probabilities: np.array,\n",
    "                       is_multidimensional: bool,\n",
    "                       search_space: np.array, # this should be type unioned with lists\\\n",
    "                       y_test: np.array,\n",
    "                       target_class_index: int = None):\n",
    "    sensitivity_scores = []\n",
    "    for i in search_space:\n",
    "        classes = convert_classes(predicted_probabilities=predicted_probabilities,\n",
    "                                  threshold=i, \n",
    "                                  is_multidimensional=is_multidimensional,\n",
    "                                  target_class_index=target_class_index)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, classes).ravel()\n",
    "        sensitivity = tp / (tp+fn)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "    print(sensitivity_scores)\n",
    "    best_sensitivity_score = max(sensitivity_scores)\n",
    "    best_sensitivity_index = sensitivity_scores.index(best_sensitivity_score)\n",
    "    best_sensitivity_threshold = search_space[best_sensitivity_index]\n",
    "    print(f'best sensitivity score: {best_sensitivity_score} occurs at threshold {best_sensitivity_threshold}')\n",
    "    return best_sensitivity_score, best_sensitivity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_specificity(predicted_probabilities: np.array,\n",
    "                       is_multidimensional: bool,\n",
    "                       search_space: np.array, # this should be type unioned with lists\\\n",
    "                       y_test: np.array,\n",
    "                       target_class_index: int = None):\n",
    "    specificity_scores = []\n",
    "    for i in search_space:\n",
    "        classes = convert_classes(predicted_probabilities=predicted_probabilities,\n",
    "                                  threshold=i, \n",
    "                                  is_multidimensional=is_multidimensional,\n",
    "                                  target_class_index=target_class_index)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, classes).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        specificity_scores.append(specificity) \n",
    "    best_specificity_score = max(specificity_scores)\n",
    "    best_specificity_index = specificity_scores.index(best_specificity_score)\n",
    "    best_specificity_threshold = search_space[best_specificity_index]\n",
    "    print(f'best specificity score: {best_specificity_score} occurs at threshold {best_specificity_threshold}')\n",
    "    return best_specificity_score, best_specificity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9917355371900827, 0.9917355371900827, 0.9917355371900827, 0.9834710743801653, 0.9834710743801653, 0.9834710743801653, 0.9834710743801653, 0.9834710743801653, 0.9834710743801653, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9752066115702479, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9669421487603306, 0.9586776859504132, 0.9504132231404959, 0.9504132231404959, 0.9504132231404959, 0.9504132231404959, 0.9504132231404959, 0.9421487603305785, 0.9421487603305785, 0.9421487603305785, 0.9421487603305785, 0.9421487603305785, 0.9421487603305785, 0.9421487603305785, 0.9338842975206612, 0.9256198347107438, 0.9090909090909091, 0.8842975206611571, 0.8842975206611571, 0.8842975206611571, 0.859504132231405, 0.8347107438016529, 0.8347107438016529, 0.8264462809917356, 0.7933884297520661, 0.768595041322314, 0.7520661157024794, 0.743801652892562, 0.6942148760330579, 0.6115702479338843, 0.4793388429752066, 0.0]\n",
      "best sensitivity score: 1.0 occurs at threshold 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_sensitivity(predicted_probabilities=predicted_probabilities,\n",
    "                    search_space=generate_search_space(),\n",
    "                    y_test=y_test,\n",
    "                  is_multidimensional=True,\n",
    "                  target_class_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "\n",
    "class ThresholdOptimizer:\n",
    "    def __init__(self,\n",
    "                 predicted_probabilities: Union[np.ndarray, pd.Series, list],\n",
    "                 y_test: Union[np.ndarray, pd.Series, list],\n",
    "                 search_space_size: int = 100):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            predicted_probabilities: output from the application of test/validation data from model/estimator.\n",
    "                This should be a list, numpy array or pandas series containing probabilities\n",
    "                that are to be converted into class predictions. If multidimensional input is given,\n",
    "                it defaults to use predictions for class 1 during optimization.\n",
    "            y_test: The true class values from the test/validation set passed into the model/estimator for predictions.\n",
    "            search_space_size: The number of possible probability threshold values to optimze for\n",
    "        \"\"\"\n",
    "        self.predicted_probabilities = predicted_probabilities\n",
    "        if len(self.predicted_probabilities.shape) == 2:\n",
    "            self.predicted_probabilities = self.predicted_probabilities[:, 1]\n",
    "        self.search_space = np.linspace(0, 1, search_space_size)\n",
    "        self.y_test = np.array(y_test)\n",
    "        self.optimized_metrics = dict()\n",
    "        self._supported_metrics = [\n",
    "            'f1', 'accuracy', 'sensitivity', 'specificity',\n",
    "            'precision', 'recall',\n",
    "        ]\n",
    "\n",
    "    def set_search_space(self,\n",
    "                         search_space_size: int):\n",
    "        \"\"\"set the number of possible probability threshold values to optimze for\n",
    "\n",
    "        This function is useful to reset the size of the search space after initializing the ThresholdOptimizer object.\n",
    "\n",
    "        Args:\n",
    "            search_space_size: The number of possible probability threshold values to optimze for\n",
    "        \"\"\"\n",
    "        self.search_space = np.linspace(0, 1, search_space_size)\n",
    "\n",
    "    def convert_classes(self,\n",
    "                        threshold: int) -> np.ndarray:\n",
    "        \"\"\"Convert predicted probabilities into binary classes based on a threshold/cutoff value\n",
    "\n",
    "        Args:\n",
    "            threshold: The probability threshold value to determine predicted classes.\n",
    "                        This follows a greater than or equal to format for determining class 1\n",
    "\n",
    "        Returns: 1 dimensional numpy array of classes\n",
    "\n",
    "        \"\"\"\n",
    "        classes = np.where(self.predicted_probabilities >= threshold, 1, 0)\n",
    "        return classes\n",
    "\n",
    "    def _get_best_metrics(self,\n",
    "                          metric_type: str,\n",
    "                          scores: list,\n",
    "                          optimization: str = 'max') -> Tuple[int, int]:\n",
    "        \"\"\"computes optimized metrics based which supported metric was specified\n",
    "\n",
    "        Args:\n",
    "            optimization:\n",
    "            metric_type:\n",
    "            scores:\n",
    "\n",
    "        Returns: best score and best threshold for a specified metric\n",
    "\n",
    "        \"\"\"\n",
    "        if optimization.lower() == 'max':\n",
    "            best_score = max(scores)\n",
    "        elif optimization.lower() == 'min':\n",
    "            best_score = min(scores)\n",
    "        else:\n",
    "            raise ValueError('Wrong value passed into optimization parameter. Should be max or min')\n",
    "        best_index = scores.index(best_score)\n",
    "        best_threshold = self.search_space[best_index]\n",
    "        self.optimized_metrics.update(\n",
    "            {\n",
    "                metric_type: {\n",
    "                    'best_score': best_score,\n",
    "                    'best_threshold': best_threshold,\n",
    "                    'all_scores': scores,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        print(f'best {metric_type}: {best_score} occurs at threshold {best_threshold}')\n",
    "        return best_score, best_threshold\n",
    "\n",
    "    def get_best_f1_metrics(self) -> Tuple[int, int]:\n",
    "        \"\"\"Optimizes threshold for F1 score\n",
    "\n",
    "        Returns: best F1 score and threshold at which best F1 score occurs\n",
    "\n",
    "        \"\"\"\n",
    "        f1_scores = list()\n",
    "        for i in self.search_space:\n",
    "            classes = self.convert_classes(threshold=i)\n",
    "            f1_scores.append(f1_score(classes, self.y_test))\n",
    "        best_f1_score, best_f1_threshold = self._get_best_metrics(\n",
    "            metric_type='f1_score',\n",
    "            scores=f1_scores,\n",
    "            optimization='max'\n",
    "        )\n",
    "        return best_f1_score, best_f1_threshold\n",
    "\n",
    "    def get_best_sensitivity_metrics(self) -> Tuple[int, int]:\n",
    "        \"\"\"Optimizes threshold for sensitivity score\n",
    "\n",
    "        Returns: best sensitivity score and threshold at which best sensitivity score occurs\n",
    "\n",
    "        \"\"\"\n",
    "        sensitivity_scores = list()\n",
    "        for i in self.search_space:\n",
    "            classes = self.convert_classes(threshold=i)\n",
    "            tn, fp, fn, tp = confusion_matrix(self.y_test, classes).ravel()\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            sensitivity_scores.append(sensitivity)\n",
    "        best_sensitivity_score, best_sensitivity_threshold = self._get_best_metrics(\n",
    "            metric_type='sensitivity_score',\n",
    "            scores=sensitivity_scores,\n",
    "            optimization='max'\n",
    "        )\n",
    "        return best_sensitivity_score, best_sensitivity_threshold\n",
    "\n",
    "    def get_best_specificity_metrics(self) -> Tuple[int, int]:\n",
    "        \"\"\"Optimizes threshold for specificity\n",
    "\n",
    "        Returns: best specificity score and threshold at which best specificity score occurs\n",
    "\n",
    "        \"\"\"\n",
    "        specificity_scores = list()\n",
    "        for i in self.search_space:\n",
    "            classes = self.convert_classes(threshold=i)\n",
    "            tn, fp, fn, tp = confusion_matrix(self.y_test, classes).ravel()\n",
    "            specificity = tn / (tn + fp)\n",
    "            specificity_scores.append(specificity)\n",
    "        best_specificity_score, best_specificity_threshold = self._get_best_metrics(\n",
    "            metric_type='specificity_score',\n",
    "            scores=specificity_scores,\n",
    "            optimization='max'\n",
    "        )\n",
    "        return best_specificity_score, best_specificity_threshold\n",
    "\n",
    "    def get_best_accuracy_metrics(self) -> Tuple[int, int]:\n",
    "        \"\"\"Optimizes threshold for accuracy\n",
    "\n",
    "        Returns: best accuracy score and threshold at which best accuracy score occurs\n",
    "\n",
    "        \"\"\"\n",
    "        accuracy_scores = list()\n",
    "        for i in self.search_space:\n",
    "            classes = self.convert_classes(threshold=i)\n",
    "            accuracy_scores.append(accuracy_score(classes, self.y_test))\n",
    "        best_accuracy_score, best_accuracy_threshold = self._get_best_metrics(\n",
    "            metric_type='accuracy_score',\n",
    "            scores=accuracy_scores,\n",
    "            optimization='max'\n",
    "        )\n",
    "        return best_accuracy_score, best_accuracy_threshold\n",
    "\n",
    "    def get_best_precision_metrics(self) -> Tuple[int, int]:\n",
    "        \"\"\"Optimizes threshold for precision\n",
    "\n",
    "        Returns: best precision score and threshold at which best precision score occurs\n",
    "\n",
    "        \"\"\"\n",
    "        precision_scores = list()\n",
    "        for i in self.search_space:\n",
    "            classes = self.convert_classes(threshold=i)\n",
    "            precision_scores.append(precision_score(classes, self.y_test))\n",
    "        best_precision_score, best_precision_threshold = self._get_best_metrics(\n",
    "            metric_type='precision_score',\n",
    "            scores=precision_scores,\n",
    "            optimization='max'\n",
    "        )\n",
    "        return best_precision_score, best_precision_threshold\n",
    "\n",
    "    def get_best_recall_metrics(self) -> Tuple[int, int]:\n",
    "        \"\"\"Optimizes threshold for recall\n",
    "\n",
    "        Returns: best recall score and threshold at which best recall score occurs\n",
    "\n",
    "        \"\"\"\n",
    "        recall_scores = list()\n",
    "        for i in self.search_space:\n",
    "            classes = self.convert_classes(threshold=i)\n",
    "            recall_scores.append(recall_score(classes, self.y_test))\n",
    "        best_recall_score, best_recall_threshold = self._get_best_metrics(\n",
    "            metric_type='precision_score',\n",
    "            scores=recall_scores,\n",
    "            optimization='max'\n",
    "        )\n",
    "        return best_recall_score, best_recall_threshold\n",
    "\n",
    "    def optimize_metrics(self,\n",
    "                         metrics: list = None):\n",
    "        \"\"\"Function to optimize for supported metrics in a batch format\n",
    "\n",
    "        Args:\n",
    "            metrics: Optional. Should be specified if only specific supported metrics are\n",
    "                    to be optimized. input must be a subset one of the supported metrics.\n",
    "                    If no metrics are applied, all metrics will be optimized for.\n",
    "\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = self._supported_metrics\n",
    "        metrics = [metric.lower() for metric in metrics]\n",
    "        assert all(metric in self._supported_metrics for metric in metrics)\n",
    "        for i in metrics:\n",
    "            super(ThresholdOptimizer, self).__getattribute__(f'get_best_{i}_metrics')()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = ThresholdOptimizer(predicted_probabilities=predicted_probabilities,\n",
    "                           y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1_score: 0.9797570850202428 occurs at threshold 0.15151515151515152\n",
      "best accuracy_score: 0.973404255319149 occurs at threshold 0.15151515151515152\n",
      "best sensitivity_score: 1.0 occurs at threshold 0.0\n",
      "best specificity_score: 1.0 occurs at threshold 0.8383838383838385\n",
      "best precision_score: 1.0 occurs at threshold 0.0\n",
      "best precision_score: 1.0 occurs at threshold 0.8383838383838385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliadjei/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "thresh.optimize_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
